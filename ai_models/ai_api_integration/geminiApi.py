import asyncio
import os

import google.generativeai as genai
from dotenv import dotenv_values

from models.paper_model import SamplePaper

env = dotenv_values(".env")


def determine_file_type(filename):
    """
    Determines the type of the file based on its extension.

    Args:
        filename (str): The name of the file.

    Returns:
        str: The type of the file, either 'pdf', 'text', or 'unknown'.
    """
    _, file_extension = os.path.splitext(filename)
    if file_extension.lower() == ".pdf":
        return "pdf"
    elif file_extension.lower() in [".txt", ".text"]:
        return "text"
    else:
        return "unknown"


async def store_file_in_local_dir(file_object, original_filename):
    """
    Stores the uploaded file in the local directory.

    Args:
        file_object (bytes): The content of the file to be stored.
        original_filename (str): The original filename of the uploaded file.

    Returns:
        str: The absolute path where the file is stored.
    """
    file_path = os.path.join(
        os.path.abspath(os.curdir), "file_upload", original_filename
    )
    with open(file_path, "wb") as out_file:
        out_file.write(file_object)
    return file_path


async def get_raw_data(file_path, file_type):
    """
    Extracts raw data from the file after uploading it to the Gemini service.

    Args:
        file_path (str): The path of the file to be processed.
        file_type (str): The type of the file, either 'pdf' or 'text'.

    Returns:
        str: The response text extracted from the file, as generated by the Gemini model.
    """
    if file_type == "pdf":
        mime_type = "application/pdf"
    elif file_type == "text":
        mime_type = "text/plain"
    files = [
        upload_to_gemini(file_path, mime_type=mime_type),
    ]
    await wait_for_files_active_async(files)

    chat_session = model.start_chat(
        history=[
            {
                "role": "user",
                "parts": [
                    files[0],
                    "chat with system",
                ],
            }
        ]
    )

    response = chat_session.send_message(
        "Extract the information from the provided file and return a JSON object using the keys defined in the Pydantic schema. Ensure the values correspond to the actual data extracted from the file."
    )

    return response.text


genai.configure(api_key=env["GEMINI_API_KEY"])


def upload_to_gemini(path, mime_type=None):
    """
    Uploads a file to the Gemini service.

    Args:
        path (str): The path to the file to be uploaded.
        mime_type (str, optional): The MIME type of the file.

    Returns:
        object: The uploaded file object returned by the Gemini service.
    """
    file = genai.upload_file(path, mime_type=mime_type)
    print(f"Uploaded file '{file.display_name}' as: {file.uri}")
    return file


async def wait_for_files_active_async(files):
    """
    Waits for the uploaded files to become active on the Gemini service.

    Args:
        files (list): A list of file objects uploaded to the Gemini service.

    Raises:
        Exception: If a file fails to process or is in a state other than 'PROCESSING' or 'ACTIVE'.
    """
    print("Waiting for file processing...")
    for file in files:
        while True:
            loop = asyncio.get_event_loop()
            file_status = await loop.run_in_executor(None, genai.get_file, file.name)
            if file_status.state.name == "ACTIVE":
                break
            elif file_status.state.name != "PROCESSING":
                raise Exception(f"File {file.name} failed to process")
            print(".", end="", flush=True)
            await asyncio.sleep(10)
    print("...all files ready")
    print()


# Create the model
generation_config = {
    "temperature": 0.3,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 8192,
    "response_mime_type": "application/json",
}

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    generation_config=generation_config,
    system_instruction=f"Extract structured information from the provided Pydantic JSON schema ({SamplePaper.model_json_schema()}) to create a JSON object that includes a list of questions with their types, answers, and parameters, along with details of the sample paper including title, type, time, and sections, while ensuring the overall response structure includes code, status, message, and data.```",
)
